# Robot Action Execution from LLM Plans

This chapter focuses on the crucial final step of the Vision-Language-Action (VLA) pipeline: translating the high-level plans generated by Large Language Models (LLMs) into concrete, executable robot movements and manipulations. This involves leveraging existing robot control frameworks and ensuring safe, reliable execution in the physical world.

## Bridging LLM Plans to Robot Control

*   **Action Primitives**: Defining a set of basic, low-level robot capabilities (e.g., "move_to_pose", "grasp_object", "open_gripper").
*   **Plan Translation**: Converting the LLM's high-level task plan (e.g., "pick up the red cube") into a sequence of these action primitives.
*   **State Machines**: Implementing state-based logic to manage the execution flow of complex robot tasks.
*   **Error Handling**: Detecting and recovering from execution failures (e.g., object not found, grasp failed).

## ROS 2 Integration for Action Execution

*   **ROS 2 Actions**: Utilizing the ROS 2 Action interface for goal-based, asynchronous task execution.
*   **MoveIt 2**: Integrating with MoveIt 2 for motion planning, inverse kinematics, and collision avoidance for robotic arms.
*   **Navigation2**: Using Nav2 for autonomous mobile robot navigation.
*   **Perception Feedback**: Incorporating real-time sensor data to refine execution and confirm task completion.

## Safety and Robustness Considerations

*   **Collision Avoidance**: Ensuring robot movements do not result in collisions with the environment or humans.
*   **Emergency Stop**: Implementing mechanisms for immediate robot shutdown.
*   **Human-Robot Collaboration**: Designing safe interaction protocols.
*   **Uncertainty Handling**: Dealing with imperfect sensor data and dynamic environments.

## Example: Executing a "Pick and Place" Plan (Conceptual ROS 2 Actions)

This conceptual example demonstrates how an LLM-generated high-level plan for a "pick and place" task can be translated into and executed using ROS 2 Action Clients. We'll simulate the interaction with an (unimplemented) ROS 2 Action Server that performs the actual robot movements. This builds upon the `examples/llm_robot_control/llm_planner.py` concept.

### 1. Conceptual ROS 2 Action Clients (`conceptual_action_executor.py`)

```python
# Conceptual Python script for executing LLM-generated plans via ROS 2 Actions
import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from action_interfaces.action import PickAndPlace, MoveArm  # Conceptual custom action types
import json
import time

# Assuming action_interfaces.action.PickAndPlace and MoveArm exist
# For demonstration, we'll simulate these.

class ConceptualActionExecutor(Node):
    def __init__(self):
        super().__init__('conceptual_action_executor')
        self.get_logger().info("Conceptual Action Executor Node Started.")

        # Conceptual Action Clients
        self._pick_and_place_action_client = ActionClient(self, PickAndPlace, 'pick_and_place_robot')
        self._move_arm_action_client = ActionClient(self, MoveArm, 'move_arm_robot')

    def send_pick_and_place_goal(self, object_name, target_location):
        self.get_logger().info(f"Sending PickAndPlace goal: {object_name} to {target_location}")
        goal_msg = PickAndPlace.Goal()
        goal_msg.object_name = object_name
        goal_msg.target_location = target_location

        # self._pick_and_place_action_client.wait_for_server() # In real system
        # self._send_goal_future = self._pick_and_place_action_client.send_goal_async(goal_msg)
        # self._send_goal_future.add_done_callback(self.pick_and_place_response_callback)
        
        # Simulate action execution
        time.sleep(2)
        self.get_logger().info(f"Simulated: Picked '{object_name}' and placed at '{target_location}'.")
        return True # Simulate success

    def send_move_arm_goal(self, target_pose):
        self.get_logger().info(f"Sending MoveArm goal to pose: {target_pose}")
        goal_msg = MoveArm.Goal()
        goal_msg.target_pose = target_pose

        # self._move_arm_action_client.wait_for_server() # In real system
        # self._send_goal_future = self._move_arm_action_client.send_goal_async(goal_msg)
        # self._send_goal_future.add_done_callback(self.move_arm_response_callback)
        
        # Simulate action execution
        time.sleep(1)
        self.get_logger().info(f"Simulated: Moved arm to {target_pose}.")
        return True # Simulate success

    def execute_llm_plan(self, llm_plan):
        self.get_logger().info(f"\nExecuting LLM Plan: {llm_plan}")
        for action in llm_plan:
            action_name = action["name"]
            params = action["params"]
            if action_name == "move_to_object":
                # In a real system, this would translate to a MoveArm action client call
                self.get_logger().info(f"Translating 'move_to_object' for {params['object_id']} to MoveArm action.")
                # self.send_move_arm_goal(get_pose_of_object(params['object_id'])) # Get pose from vision
                time.sleep(1) # Simulate movement
            elif action_name == "grasp_object":
                self.get_logger().info(f"Translating 'grasp_object' for {params['object_id']} to PickAndPlace action (grasp part).")
                # self.send_pick_and_place_goal(params['object_id'], "current_location") # Just grasp
                time.sleep(1) # Simulate grasp
            elif action_name == "move_to_location":
                self.get_logger().info(f"Translating 'move_to_location' for {params['location']} to MoveArm action (navigate part).")
                # self.send_move_arm_goal(get_pose_of_location(params['location'])) # Get pose of location
                time.sleep(2) # Simulate movement
            elif action_name == "pick_and_place": # Hypothetical combined action
                self.send_pick_and_place_goal(params['object_id'], params['target_location'])
            else:
                self.get_logger().warning(f"Unknown action: {action_name}")
            
        self.get_logger().info("LLM Plan execution complete.")

# Placeholder for conceptual Action interfaces
class PickAndPlace:
    class Goal:
        object_name = ""
        target_location = ""
    # Result, Feedback classes would also be here

class MoveArm:
    class Goal:
        target_pose = {} # e.g., {'x': 0, 'y': 0, 'z': 0}
    # Result, Feedback classes would also be here


def main(args=None):
    rclpy.init(args=args)
    action_executor = ConceptualActionExecutor()

    # Simulate an LLM-generated plan (from examples/llm_robot_control/llm_planner.py)
    llm_plan_example = [
        {"name": "move_to_object", "params": {"object_id": "red_ball"}},
        {"name": "grasp_object", "params": {"object_id": "red_ball"}},
        {"name": "move_to_location", "params": {"location": "drop_zone"}}
    ]

    action_executor.execute_llm_plan(llm_plan_example)
    
    action_executor.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Explanation

This script demonstrates the Action Execution module's role:
1.  **Conceptual Action Clients**: It sets up `ActionClient` objects for hypothetical `PickAndPlace` and `MoveArm` ROS 2 actions. In a real system, these would connect to actual Action Servers on the robot.
2.  **`execute_llm_plan()`**: This function iterates through a simulated LLM plan (a list of high-level actions like "move_to_object", "grasp_object"). For each high-level action, it conceptually translates it into a call to the appropriate ROS 2 Action Client.
3.  **Simulated Action Execution**: Instead of actual robot movement, `time.sleep()` is used to simulate the duration of these actions, and `self.get_logger().info()` prints messages about the simulated execution.

This module acts as the bridge, converting the LLM's abstract intent into concrete physical movements by calling the robot's pre-defined capabilities via ROS 2 Actions.

### How to Run

1.  Save the above content as `conceptual_action_executor.py` (e.g., in `examples/llm_robot_control/`).
2.  Ensure you have a ROS 2 environment sourced.
3.  Run the script from your terminal:
    ```bash
    python3 conceptual_action_executor.py
    ```
You will see the simulated execution of the LLM-generated plan, showing the translation of high-level actions into conceptual robot commands.

### Reference

*   `examples/llm_robot_control/llm_planner.py` (provides the LLM plan example)
