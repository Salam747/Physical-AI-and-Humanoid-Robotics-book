# Reinforcement Learning Basics for Robotics

This chapter introduces the fundamental concepts of Reinforcement Learning (RL) and its application in robotics. You will learn how to formulate robotic control problems as RL tasks, understand the core components of an RL system, and explore various algorithms used to train intelligent agents for complex robotic behaviors like walking and grasping.

## What is Reinforcement Learning?

*   **Agent-Environment Interaction**: Learning through trial and error.
*   **State, Action, Reward**: Defining the key elements of an RL problem.
*   **Policy**: The agent's strategy for choosing actions.
*   **Value Function**: Estimating the long-term return of states or actions.

## Key RL Algorithms

*   **Model-Free vs. Model-Based**: Learning directly from experience vs. learning a model of the environment.
*   **Value-Based Methods**: Q-learning, SARSA, Deep Q-Networks (DQN).
*   **Policy-Based Methods**: Policy Gradients, REINFORCE.
*   **Actor-Critic Methods**: Advantage Actor-Critic (A2C), Proximal Policy Optimization (PPO).

## RL in Robotics

*   **Motion Control**: Learning to walk, run, and balance for humanoid robots.
*   **Manipulation**: Training robotic arms to grasp and manipulate objects.
*   **Navigation**: Developing intelligent navigation strategies in dynamic environments.
*   **Challenges**: High-dimensional state/action spaces, sparse rewards, safety.

## Example: Simple Robotic Arm Grasping (Conceptual RL Environment)

This conceptual example outlines how you would define a reinforcement learning environment for a simple robotic arm to learn a grasping task, typically within a simulator like NVIDIA Isaac Sim. We'll conceptualize the key components: observation space, action space, reward function, and episode termination.

### 1. Conceptual RL Environment Setup

We'll define the environment based on typical Gymnasium (formerly OpenAI Gym) style interfaces, adapted for a robotics simulator.

#### `conceptual_grasping_env.py`

```python
# Conceptual RL Environment for a Robotic Arm Grasping Task
import gymnasium as gym
from gymnasium import spaces
import numpy as np
# from omni.isaac.core import World # Conceptual Isaac Sim integration

class ConceptualIsaacGraspingEnv(gym.Env):
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}

    def __init__(self, render_mode=None):
        super().__init__()
        
        # --- Action Space ---
        # For a simple arm, actions might be changes in end-effector position (dx, dy, dz)
        # and a gripper command (open/close).
        # Assuming a continuous action space for end-effector movements (3D velocity)
        # and a discrete action for gripper (0: open, 1: close).
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(4,), dtype=np.float32)
        # Action vector: [delta_x, delta_y, delta_z, gripper_state]
        
        # --- Observation Space ---
        # Observations could include:
        # - End-effector pose (position and orientation)
        # - Object pose (position and orientation of the target object)
        # - Joint angles of the arm
        # - Image from a depth camera (high-dimensional)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(20,), dtype=np.float32) 
        # Example: [ee_pose (7), object_pose (7), joint_angles (6)]

        # --- Isaac Sim World (Conceptual) ---
        # self.world = World(stage_units_in_meters=1.0)
        # self.robot = self.world.scene.get_object("franka_robot") # Or your humanoid arm
        # self.target_object = self.world.scene.get_object("target_cube")

        self.render_mode = render_mode
        # self.viewer = None # For rendering if self.render_mode == "human"

    def _get_obs(self):
        # Conceptual observation retrieval from simulator
        # ee_pose = self.robot.end_effector_pose()
        # object_pose = self.target_object.pose()
        # joint_angles = self.robot.get_joint_positions()
        # return np.concatenate([ee_pose, object_pose, joint_angles])
        return np.random.rand(20).astype(np.float32) # Placeholder for actual observation

    def _get_info(self):
        return {"distance_to_object": np.random.rand()} # Placeholder info

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # Reset simulator here (e.g., reset robot pose, randomize object position)
        # self.world.reset()
        
        observation = self._get_obs()
        info = self._get_info()
        return observation, info

    def step(self, action):
        # Apply action to simulator (e.g., move end-effector, change gripper state)
        # self.robot.apply_action(action)
        # self.world.step() # Advance simulation by one step

        observation = self._get_obs()
        reward = self._compute_reward()
        terminated = self._check_termination()
        truncated = self._check_truncation() # e.g., episode timeout
        info = self._get_info()

        return observation, reward, terminated, truncated, info

    def _compute_reward(self):
        # Reward function: higher for closer to object, positive for grasping
        # distance_reward = -np.linalg.norm(self.robot.end_effector_pos - self.target_object.pos)
        # grasp_reward = 10.0 if self.robot.is_grasping(self.target_object) else 0.0
        return np.random.rand() # Placeholder reward

    def _check_termination(self):
        # Termination condition: object grasped, or object dropped, or episode success
        # return self.robot.is_grasping(self.target_object) or self.robot.object_dropped()
        return np.random.rand() > 0.95 # Placeholder termination

    def _check_truncation(self):
        # Truncation: episode timeout
        # return self.world.current_time_step_index > self.max_episode_steps
        return np.random.rand() > 0.99 # Placeholder truncation

    def close(self):
        # self.world.shutdown() # Shutdown simulator
        if self.viewer:
            self.viewer.close()

if __name__ == '__main__':
    print("--- Conceptual Grasping Environment ---")
    env = ConceptualIsaacGraspingEnv()
    obs, info = env.reset()
    print("Initial Observation Shape:", obs.shape)
    print("Initial Info:", info)

    for _ in range(5):
        action = env.action_space.sample() # Random action
        obs, reward, terminated, truncated, info = env.step(action)
        print(f"Step: Reward={reward:.2f}, Terminated={terminated}, Truncated={truncated}")
        if terminated or truncated:
            break
    env.close()
```

### Explanation

This conceptual Python code defines a basic RL environment:
1.  **`gym.Env` Structure**: It inherits from `gym.Env` to provide a standardized interface for RL agents.
2.  **Action Space**: Defines actions as continuous end-effector movements and a discrete gripper command.
3.  **Observation Space**: Specifies the types of information the agent receives (e.g., robot pose, object pose, joint angles).
4.  **`reset()`**: Resets the simulation to an initial state, often randomizing object positions.
5.  **`step(action)`**: Applies an action to the simulated robot, advances the simulation, and returns the next observation, reward, and termination flags.
6.  **`_compute_reward()`**: Defines how the agent is rewarded for its actions (e.g., positive for grasping, negative for distance).

### High-Level Algorithm (PPO - Proximal Policy Optimization)

The `examples/rl_sim_to_real/rl_environment_config.yaml` suggests using PPO. A typical PPO training loop would conceptually look like this:

1.  **Initialize**: Create the `ConceptualIsaacGraspingEnv` and a PPO agent.
2.  **Loop Episodes**:
    *   `observation, info = env.reset()`
    *   **Loop Timesteps**:
        *   `action = agent.select_action(observation)` (based on current policy)
        *   `next_observation, reward, terminated, truncated, info = env.step(action)`
        *   `agent.store_experience(observation, action, reward, next_observation, terminated)`
        *   `observation = next_observation`
        *   If enough experience collected, `agent.update_policy()` (using PPO algorithm)
        *   If `terminated` or `truncated`, break inner loop.
3.  **Evaluate**: Periodically evaluate the agent's performance.

### Reference

*   `examples/rl_sim_to_real/rl_environment_config.yaml`
