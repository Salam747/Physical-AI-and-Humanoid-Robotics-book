# Capstone Project: End-to-End Vision-Language-Action (VLA) Robot

This repository contains the demonstration project for the Capstone of the "Physical AI and Humanoid Robotics" book. It showcases an end-to-end VLA system enabling a robot to execute natural language commands.

## Overview

The project integrates:
*   ROS 2 for robot control and communication.
*   NVIDIA Isaac Sim for realistic simulation (Digital Twin).
*   Isaac ROS GEMs for perception and navigation (AI-Robot Brain).
*   A Large Language Model (LLM) for high-level planning and task decomposition.

## Setup

(Instructions for setting up the environment, including ROS 2, Isaac Sim, and LLM API keys, will go here.)

## Usage

(Instructions for running the demonstration, providing natural language commands, and observing robot behavior will go here.)

## Structure

*   `robot_ws/`: ROS 2 workspace containing robot control nodes, action servers.
*   `llm_interface/`: Python package for LLM interaction and plan translation.
*   `isaac_sim_assets/`: Custom assets or scene descriptions for Isaac Sim.
