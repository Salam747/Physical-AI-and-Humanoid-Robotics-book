"use strict";(globalThis.webpackChunkrobotic_book=globalThis.webpackChunkrobotic_book||[]).push([[514],{2552:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Module 1 \u2013 The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module1-ros2/","label":"Module 1 \u2013 The Robotic Nervous System (ROS 2)","docId":"module1-ros2/index","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module1-ros2/ros2-architecture","label":"ROS 2 Architecture","docId":"module1-ros2/ros2-architecture","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module1-ros2/rclpy-development","label":"Real-time Python Development with rclpy","docId":"module1-ros2/rclpy-development","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module1-ros2/custom-messages","label":"Creating and Publishing Custom ROS 2 Messages","docId":"module1-ros2/custom-messages","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module1-ros2/ros2-packages","label":"Building Reusable ROS 2 Packages and Workspaces","docId":"module1-ros2/ros2-packages","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module1-ros2/urdf-xacro-modeling","label":"URDF & Xacro Modeling for Humanoid Robots","docId":"module1-ros2/urdf-xacro-modeling","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module1-ros2/launch-files-rviz","label":"Launch Files, Parameters, and RViz2 Debugging","docId":"module1-ros2/launch-files-rviz","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module1-ros2/bridging-external-agents","label":"Bridging External Python Agents to ROS 2 Controllers","docId":"module1-ros2/bridging-external-agents","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2 \u2013 The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module2-digital-twin/","label":"Module 2 \u2013 The Digital Twin (Gazebo & Unity)","docId":"module2-digital-twin/index","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module2-digital-twin/digital-twin-creation","label":"Digital Twin Creation (Overview)","docId":"module2-digital-twin/digital-twin-creation","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module2-digital-twin/urdf-sdf-conversion","label":"URDF to SDF Conversion","docId":"module2-digital-twin/urdf-sdf-conversion","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module2-digital-twin/physics-tuning","label":"Realistic Physics Tuning","docId":"module2-digital-twin/physics-tuning","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module2-digital-twin/sensor-simulation","label":"Sensor Simulation","docId":"module2-digital-twin/sensor-simulation","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module2-digital-twin/high-fidelity-environments","label":"High-Fidelity Environments (Isaac Sim + Omniverse)","docId":"module2-digital-twin/high-fidelity-environments","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module2-digital-twin/unity-visualization","label":"Unity for Visualization and HRI Studies","docId":"module2-digital-twin/unity-visualization","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3 \u2013 The AI-Robot Brain (NVIDIA Isaac\u2122)","items":[{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/","label":"Module 3 \u2013 The AI-Robot Brain (NVIDIA Isaac\u2122)","docId":"module3-ai-brain/index","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/isaac-sim-intro","label":"Isaac Sim Introduction","docId":"module3-ai-brain/isaac-sim-intro","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/domain-randomization","label":"Domain Randomization and Synthetic Data","docId":"module3-ai-brain/domain-randomization","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/isaac-ros-gems-slam","label":"Isaac ROS GEMs for Visual SLAM","docId":"module3-ai-brain/isaac-ros-gems-slam","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/isaac-ros-gems-perception","label":"Isaac ROS GEMs for AprilTag and PeopleNet","docId":"module3-ai-brain/isaac-ros-gems-perception","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/nav2-bipedal","label":"Nav2 Configuration for Unstable Bipedal Platforms","docId":"module3-ai-brain/nav2-bipedal","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/reinforcement-learning-basics","label":"Reinforcement Learning Basics for Robotics","docId":"module3-ai-brain/reinforcement-learning-basics","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/sim-to-real","label":"Sim-to-Real Transfer Recipes","docId":"module3-ai-brain/sim-to-real","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4 \u2013 Vision-Language-Action (VLA)","items":[{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/","label":"Module 4 \u2013 Vision-Language-Action (VLA)","docId":"module4-vla/index","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/vla-pipeline-architecture","label":"VLA Pipeline Architecture","docId":"module4-vla/vla-pipeline-architecture","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/llm-integration-for-planning","label":"LLM Integration for High-Level Planning","docId":"module4-vla/llm-integration-for-planning","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/vision-language-processing","label":"Vision and Language Processing for VLA","docId":"module4-vla/vision-language-processing","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/robot-action-execution","label":"Robot Action Execution from LLM Plans","docId":"module4-vla/robot-action-execution","unlisted":false},{"type":"link","href":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/capstone-project","label":"Capstone Project: End-to-End VLA System","docId":"module4-vla/capstone-project","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"module1-ros2/bridging-external-agents":{"id":"module1-ros2/bridging-external-agents","title":"Bridging External Python Agents to ROS 2 Controllers","description":"This chapter explores how to integrate external Python applications, including advanced AI agents like Large Language Models (LLMs), with your ROS 2 robot control system. You will learn to design communication interfaces that enable high-level decision-making processes to influence real-time robot behavior.","sidebar":"tutorialSidebar"},"module1-ros2/custom-messages":{"id":"module1-ros2/custom-messages","title":"Creating and Publishing Custom ROS 2 Messages","description":"This chapter guides you through the process of defining, building, and using custom message types in ROS 2. Custom messages are essential for exchanging domain-specific data between nodes in your robotics application.","sidebar":"tutorialSidebar"},"module1-ros2/index":{"id":"module1-ros2/index","title":"Module 1 \u2013 The Robotic Nervous System (ROS 2)","description":"This module introduces ROS 2, the industry-standard middleware for robot control. You will master ROS 2 architecture, real-time Python development with rclpy, custom message creation, and building reusable packages. By the end, you\'ll have a production-grade ROS 2 workspace capable of controlling simulated humanoid robots.","sidebar":"tutorialSidebar"},"module1-ros2/launch-files-rviz":{"id":"module1-ros2/launch-files-rviz","title":"Launch Files, Parameters, and RViz2 Debugging","description":"This chapter covers the essential tools and techniques for orchestrating ROS 2 applications, managing runtime configurations, and visualizing your robot in a simulated environment using RViz2.","sidebar":"tutorialSidebar"},"module1-ros2/rclpy-development":{"id":"module1-ros2/rclpy-development","title":"Real-time Python Development with rclpy","description":"This chapter focuses on developing real-time ROS 2 applications using rclpy, the Python client library for ROS 2. You will learn best practices for writing efficient and predictable Python code in a robotics context.","sidebar":"tutorialSidebar"},"module1-ros2/ros2-architecture":{"id":"module1-ros2/ros2-architecture","title":"ROS 2 Architecture","description":"This chapter provides a comprehensive overview of the ROS 2 (Robot Operating System 2) architecture, its core concepts, and how it addresses the challenges of modern robotics development.","sidebar":"tutorialSidebar"},"module1-ros2/ros2-packages":{"id":"module1-ros2/ros2-packages","title":"Building Reusable ROS 2 Packages and Workspaces","description":"This chapter details the process of structuring your ROS 2 projects into reusable packages and managing them within a colcon workspace. A well-organized workspace is crucial for collaborative development, code sharing, and efficient build processes.","sidebar":"tutorialSidebar"},"module1-ros2/urdf-xacro-modeling":{"id":"module1-ros2/urdf-xacro-modeling","title":"URDF & Xacro Modeling for Humanoid Robots","description":"This chapter delves into the Unified Robot Description Format (URDF) and its powerful extension, Xacro. You will learn how to create detailed and flexible models of humanoid robots, specifying their kinematics, dynamics, visual properties, and collision geometries.","sidebar":"tutorialSidebar"},"module2-digital-twin/digital-twin-creation":{"id":"module2-digital-twin/digital-twin-creation","title":"Digital Twin Creation (Overview)","description":"This chapter provides an overview of digital twins in the context of robotics, focusing on their benefits, key components, and the process of their creation. A digital twin is a virtual representation of a physical system, enabling simulation, testing, and optimization without interacting with real hardware.","sidebar":"tutorialSidebar"},"module2-digital-twin/high-fidelity-environments":{"id":"module2-digital-twin/high-fidelity-environments","title":"High-Fidelity Environments (Isaac Sim + Omniverse)","description":"This chapter delves into creating and utilizing high-fidelity simulation environments for robotics, leveraging advanced platforms like NVIDIA Isaac Sim and the Omniverse ecosystem. These photorealistic environments are crucial for training AI agents and testing complex robotic behaviors in a visually and physically accurate setting.","sidebar":"tutorialSidebar"},"module2-digital-twin/index":{"id":"module2-digital-twin/index","title":"Module 2 \u2013 The Digital Twin (Gazebo & Unity)","description":"This module explores the creation and utilization of photorealistic digital twins for robotics. You will master concepts like URDF to SDF conversion, realistic physics tuning, sensor simulation, and deploying humanoids in high-fidelity environments using tools like NVIDIA Isaac Sim and Unity.","sidebar":"tutorialSidebar"},"module2-digital-twin/physics-tuning":{"id":"module2-digital-twin/physics-tuning","title":"Realistic Physics Tuning","description":"This chapter focuses on tuning the physics parameters of your digital twin to achieve realistic and accurate simulation results. Proper physics tuning is critical for ensuring that robot behaviors developed in simulation transfer effectively to the real world.","sidebar":"tutorialSidebar"},"module2-digital-twin/sensor-simulation":{"id":"module2-digital-twin/sensor-simulation","title":"Sensor Simulation","description":"This chapter explores the crucial aspect of simulating robot sensors to generate data that closely mimics real-world hardware. Accurate sensor simulation is vital for developing and testing perception algorithms, ensuring that software trained in simulation performs reliably on physical robots.","sidebar":"tutorialSidebar"},"module2-digital-twin/unity-visualization":{"id":"module2-digital-twin/unity-visualization","title":"Unity for Visualization and HRI Studies","description":"This chapter explores the use of Unity, a powerful real-time 3D development platform, for advanced visualization and Human-Robot Interaction (HRI) studies within the context of digital twins. Unity offers robust rendering capabilities, physics, and an extensive asset store, making it an excellent choice for creating compelling robotic simulations and user interfaces.","sidebar":"tutorialSidebar"},"module2-digital-twin/urdf-sdf-conversion":{"id":"module2-digital-twin/urdf-sdf-conversion","title":"URDF to SDF Conversion","description":"This chapter details the process of converting robot models from Universal Robot Description Format (URDF) to Simulation Description Format (SDF). While URDF is excellent for describing the kinematic and dynamic properties of a single robot, SDF provides a more comprehensive representation for physics simulators like Gazebo, enabling multi-robot scenarios, environmental elements, and a richer set of physical properties.","sidebar":"tutorialSidebar"},"module3-ai-brain/domain-randomization":{"id":"module3-ai-brain/domain-randomization","title":"Domain Randomization and Synthetic Data","description":"This chapter explores the powerful techniques of domain randomization and synthetic data generation, essential for training robust AI models for robotics. By generating varied data in simulation, we can improve the ability of trained models to generalize to the complexities and uncertainties of the real world.","sidebar":"tutorialSidebar"},"module3-ai-brain/index":{"id":"module3-ai-brain/index","title":"Module 3 \u2013 The AI-Robot Brain (NVIDIA Isaac\u2122)","description":"This module delves into building the \\"AI-Robot Brain\\" using NVIDIA Isaac Sim and Isaac ROS. You will master GPU-accelerated perception, navigation, and manipulation techniques, including domain randomization, Isaac ROS GEMs (Visual SLAM, AprilTag, PeopleNet), reinforcement learning for robotics, and effective sim-to-real transfer strategies.","sidebar":"tutorialSidebar"},"module3-ai-brain/isaac-ros-gems-perception":{"id":"module3-ai-brain/isaac-ros-gems-perception","title":"Isaac ROS GEMs for AprilTag and PeopleNet","description":"This chapter continues our exploration of NVIDIA Isaac ROS GEMs, focusing on their application in specific perception tasks: AprilTag detection for robust localization and object tracking, and PeopleNet/Pose estimation for human detection and understanding. These GPU-accelerated modules are crucial for enabling robots to interact intelligently and safely with their environment.","sidebar":"tutorialSidebar"},"module3-ai-brain/isaac-ros-gems-slam":{"id":"module3-ai-brain/isaac-ros-gems-slam","title":"Isaac ROS GEMs for Visual SLAM","description":"This chapter introduces NVIDIA Isaac ROS GEMs, focusing specifically on those designed for Visual SLAM (Simultaneous Localization and Mapping). These GPU-accelerated ROS 2 packages provide high-performance solutions for robots to build 3D maps of their environment while simultaneously tracking their own position within that map.","sidebar":"tutorialSidebar"},"module3-ai-brain/isaac-sim-intro":{"id":"module3-ai-brain/isaac-sim-intro","title":"Isaac Sim Introduction","description":"This chapter introduces NVIDIA Isaac Sim, a powerful robotics simulation and development platform built on the NVIDIA Omniverse. Isaac Sim provides a highly realistic and physically accurate environment for developing, testing, and training AI-powered robots, bridging the gap between simulation and the real world.","sidebar":"tutorialSidebar"},"module3-ai-brain/nav2-bipedal":{"id":"module3-ai-brain/nav2-bipedal","title":"Nav2 Configuration for Unstable Bipedal Platforms","description":"This chapter addresses the challenges of configuring the ROS 2 Navigation Stack (Nav2) for bipedal robots. Unlike wheeled or tracked robots, bipedal platforms introduce significant complexities related to stability, balance, and dynamic motion. You will learn how to adapt Nav2\'s components to enable autonomous navigation for walking robots.","sidebar":"tutorialSidebar"},"module3-ai-brain/reinforcement-learning-basics":{"id":"module3-ai-brain/reinforcement-learning-basics","title":"Reinforcement Learning Basics for Robotics","description":"This chapter introduces the fundamental concepts of Reinforcement Learning (RL) and its application in robotics. You will learn how to formulate robotic control problems as RL tasks, understand the core components of an RL system, and explore various algorithms used to train intelligent agents for complex robotic behaviors like walking and grasping.","sidebar":"tutorialSidebar"},"module3-ai-brain/sim-to-real":{"id":"module3-ai-brain/sim-to-real","title":"Sim-to-Real Transfer Recipes","description":"This chapter focuses on the critical problem of Sim-to-Real (S2R) transfer in robotics \u2013 how to successfully deploy policies or behaviors learned in simulation to physical robots. You will learn practical recipes and techniques to minimize the \\"reality gap\\" and ensure your AI models perform reliably in the real world.","sidebar":"tutorialSidebar"},"module4-vla/capstone-project":{"id":"module4-vla/capstone-project","title":"Capstone Project: End-to-End VLA System","description":"This chapter culminates your journey through the Physical AI and Humanoid Robotics book with a comprehensive capstone project. You will integrate all the concepts learned across Modules 1 to 4 \u2013 ROS 2 fundamentals, digital twin creation, AI-Robot Brain (perception, navigation, manipulation), and Vision-Language-Action (VLA) pipelines \u2013 to build a fully functional end-to-end VLA system capable of controlling a physical robot based on natural language commands.","sidebar":"tutorialSidebar"},"module4-vla/index":{"id":"module4-vla/index","title":"Module 4 \u2013 Vision-Language-Action (VLA)","description":"This module is where Large Language Models (LLMs) finally control physical robots through Vision-Language-Action (VLA) pipelines. You will learn to build a complete pipeline for LLM-driven robot control, integrating vision processing, language understanding, and robot action execution, culminating in a capstone project.","sidebar":"tutorialSidebar"},"module4-vla/llm-integration-for-planning":{"id":"module4-vla/llm-integration-for-planning","title":"LLM Integration for High-Level Planning","description":"This chapter explores how Large Language Models (LLMs) can be integrated into a Vision-Language-Action (VLA) pipeline to perform high-level task planning for robots. The focus is on translating abstract natural language commands into a sequence of executable robot actions.","sidebar":"tutorialSidebar"},"module4-vla/robot-action-execution":{"id":"module4-vla/robot-action-execution","title":"Robot Action Execution from LLM Plans","description":"This chapter focuses on the crucial final step of the Vision-Language-Action (VLA) pipeline: translating the high-level plans generated by Large Language Models (LLMs) into concrete, executable robot movements and manipulations. This involves leveraging existing robot control frameworks and ensuring safe, reliable execution in the physical world.","sidebar":"tutorialSidebar"},"module4-vla/vision-language-processing":{"id":"module4-vla/vision-language-processing","title":"Vision and Language Processing for VLA","description":"This chapter delves into the crucial intersection of computer vision and natural language processing within Vision-Language-Action (VLA) pipelines. The goal is to enable robots to \\"see\\" and \\"understand\\" their environment, and then link those perceptions to human language commands, facilitating effective interaction and task execution.","sidebar":"tutorialSidebar"},"module4-vla/vla-pipeline-architecture":{"id":"module4-vla/vla-pipeline-architecture","title":"VLA Pipeline Architecture","description":"This chapter introduces the Vision-Language-Action (VLA) pipeline, a cutting-edge architectural approach for enabling Large Language Models (LLMs) to understand the physical world through vision, process natural language commands, and execute complex actions on robots.","sidebar":"tutorialSidebar"}}}}')}}]);