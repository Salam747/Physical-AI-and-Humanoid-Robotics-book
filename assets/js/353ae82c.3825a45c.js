"use strict";(globalThis.webpackChunkrobotic_book=globalThis.webpackChunkrobotic_book||[]).push([[281],{7083:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module3-ai-brain/reinforcement-learning-basics","title":"Reinforcement Learning Basics for Robotics","description":"This chapter introduces the fundamental concepts of Reinforcement Learning (RL) and its application in robotics. You will learn how to formulate robotic control problems as RL tasks, understand the core components of an RL system, and explore various algorithms used to train intelligent agents for complex robotic behaviors like walking and grasping.","source":"@site/docs/module3-ai-brain/06-reinforcement-learning-basics.mdx","sourceDirName":"module3-ai-brain","slug":"/module3-ai-brain/reinforcement-learning-basics","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/reinforcement-learning-basics","draft":false,"unlisted":false,"editUrl":"https://github.com/yourusername/physical-ai-humanoid-book/tree/main/docs/module3-ai-brain/06-reinforcement-learning-basics.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 Configuration for Unstable Bipedal Platforms","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/nav2-bipedal"},"next":{"title":"Sim-to-Real Transfer Recipes","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/sim-to-real"}}');var i=r(4848),t=r(8453);const s={},a="Reinforcement Learning Basics for Robotics",l={},c=[{value:"What is Reinforcement Learning?",id:"what-is-reinforcement-learning",level:2},{value:"Key RL Algorithms",id:"key-rl-algorithms",level:2},{value:"RL in Robotics",id:"rl-in-robotics",level:2},{value:"Example: Simple Robotic Arm Grasping (Conceptual RL Environment)",id:"example-simple-robotic-arm-grasping-conceptual-rl-environment",level:2},{value:"1. Conceptual RL Environment Setup",id:"1-conceptual-rl-environment-setup",level:3},{value:"<code>conceptual_grasping_env.py</code>",id:"conceptual_grasping_envpy",level:4},{value:"Explanation",id:"explanation",level:3},{value:"High-Level Algorithm (PPO - Proximal Policy Optimization)",id:"high-level-algorithm-ppo---proximal-policy-optimization",level:3},{value:"Reference",id:"reference",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"reinforcement-learning-basics-for-robotics",children:"Reinforcement Learning Basics for Robotics"})}),"\n",(0,i.jsx)(n.p,{children:"This chapter introduces the fundamental concepts of Reinforcement Learning (RL) and its application in robotics. You will learn how to formulate robotic control problems as RL tasks, understand the core components of an RL system, and explore various algorithms used to train intelligent agents for complex robotic behaviors like walking and grasping."}),"\n",(0,i.jsx)(n.h2,{id:"what-is-reinforcement-learning",children:"What is Reinforcement Learning?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Agent-Environment Interaction"}),": Learning through trial and error."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State, Action, Reward"}),": Defining the key elements of an RL problem."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Policy"}),": The agent's strategy for choosing actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Value Function"}),": Estimating the long-term return of states or actions."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-rl-algorithms",children:"Key RL Algorithms"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model-Free vs. Model-Based"}),": Learning directly from experience vs. learning a model of the environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Value-Based Methods"}),": Q-learning, SARSA, Deep Q-Networks (DQN)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Policy-Based Methods"}),": Policy Gradients, REINFORCE."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actor-Critic Methods"}),": Advantage Actor-Critic (A2C), Proximal Policy Optimization (PPO)."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"rl-in-robotics",children:"RL in Robotics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Motion Control"}),": Learning to walk, run, and balance for humanoid robots."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation"}),": Training robotic arms to grasp and manipulate objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation"}),": Developing intelligent navigation strategies in dynamic environments."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Challenges"}),": High-dimensional state/action spaces, sparse rewards, safety."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"example-simple-robotic-arm-grasping-conceptual-rl-environment",children:"Example: Simple Robotic Arm Grasping (Conceptual RL Environment)"}),"\n",(0,i.jsx)(n.p,{children:"This conceptual example outlines how you would define a reinforcement learning environment for a simple robotic arm to learn a grasping task, typically within a simulator like NVIDIA Isaac Sim. We'll conceptualize the key components: observation space, action space, reward function, and episode termination."}),"\n",(0,i.jsx)(n.h3,{id:"1-conceptual-rl-environment-setup",children:"1. Conceptual RL Environment Setup"}),"\n",(0,i.jsx)(n.p,{children:"We'll define the environment based on typical Gymnasium (formerly OpenAI Gym) style interfaces, adapted for a robotics simulator."}),"\n",(0,i.jsx)(n.h4,{id:"conceptual_grasping_envpy",children:(0,i.jsx)(n.code,{children:"conceptual_grasping_env.py"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Conceptual RL Environment for a Robotic Arm Grasping Task\r\nimport gymnasium as gym\r\nfrom gymnasium import spaces\r\nimport numpy as np\r\n# from omni.isaac.core import World # Conceptual Isaac Sim integration\r\n\r\nclass ConceptualIsaacGraspingEnv(gym.Env):\r\n    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}\r\n\r\n    def __init__(self, render_mode=None):\r\n        super().__init__()\r\n        \r\n        # --- Action Space ---\r\n        # For a simple arm, actions might be changes in end-effector position (dx, dy, dz)\r\n        # and a gripper command (open/close).\r\n        # Assuming a continuous action space for end-effector movements (3D velocity)\r\n        # and a discrete action for gripper (0: open, 1: close).\r\n        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(4,), dtype=np.float32)\r\n        # Action vector: [delta_x, delta_y, delta_z, gripper_state]\r\n        \r\n        # --- Observation Space ---\r\n        # Observations could include:\r\n        # - End-effector pose (position and orientation)\r\n        # - Object pose (position and orientation of the target object)\r\n        # - Joint angles of the arm\r\n        # - Image from a depth camera (high-dimensional)\r\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(20,), dtype=np.float32) \r\n        # Example: [ee_pose (7), object_pose (7), joint_angles (6)]\r\n\r\n        # --- Isaac Sim World (Conceptual) ---\r\n        # self.world = World(stage_units_in_meters=1.0)\r\n        # self.robot = self.world.scene.get_object("franka_robot") # Or your humanoid arm\r\n        # self.target_object = self.world.scene.get_object("target_cube")\r\n\r\n        self.render_mode = render_mode\r\n        # self.viewer = None # For rendering if self.render_mode == "human"\r\n\r\n    def _get_obs(self):\r\n        # Conceptual observation retrieval from simulator\r\n        # ee_pose = self.robot.end_effector_pose()\r\n        # object_pose = self.target_object.pose()\r\n        # joint_angles = self.robot.get_joint_positions()\r\n        # return np.concatenate([ee_pose, object_pose, joint_angles])\r\n        return np.random.rand(20).astype(np.float32) # Placeholder for actual observation\r\n\r\n    def _get_info(self):\r\n        return {"distance_to_object": np.random.rand()} # Placeholder info\r\n\r\n    def reset(self, seed=None, options=None):\r\n        super().reset(seed=seed)\r\n        # Reset simulator here (e.g., reset robot pose, randomize object position)\r\n        # self.world.reset()\r\n        \r\n        observation = self._get_obs()\r\n        info = self._get_info()\r\n        return observation, info\r\n\r\n    def step(self, action):\r\n        # Apply action to simulator (e.g., move end-effector, change gripper state)\r\n        # self.robot.apply_action(action)\r\n        # self.world.step() # Advance simulation by one step\r\n\r\n        observation = self._get_obs()\r\n        reward = self._compute_reward()\r\n        terminated = self._check_termination()\r\n        truncated = self._check_truncation() # e.g., episode timeout\r\n        info = self._get_info()\r\n\r\n        return observation, reward, terminated, truncated, info\r\n\r\n    def _compute_reward(self):\r\n        # Reward function: higher for closer to object, positive for grasping\r\n        # distance_reward = -np.linalg.norm(self.robot.end_effector_pos - self.target_object.pos)\r\n        # grasp_reward = 10.0 if self.robot.is_grasping(self.target_object) else 0.0\r\n        return np.random.rand() # Placeholder reward\r\n\r\n    def _check_termination(self):\r\n        # Termination condition: object grasped, or object dropped, or episode success\r\n        # return self.robot.is_grasping(self.target_object) or self.robot.object_dropped()\r\n        return np.random.rand() > 0.95 # Placeholder termination\r\n\r\n    def _check_truncation(self):\r\n        # Truncation: episode timeout\r\n        # return self.world.current_time_step_index > self.max_episode_steps\r\n        return np.random.rand() > 0.99 # Placeholder truncation\r\n\r\n    def close(self):\r\n        # self.world.shutdown() # Shutdown simulator\r\n        if self.viewer:\r\n            self.viewer.close()\r\n\r\nif __name__ == \'__main__\':\r\n    print("--- Conceptual Grasping Environment ---")\r\n    env = ConceptualIsaacGraspingEnv()\r\n    obs, info = env.reset()\r\n    print("Initial Observation Shape:", obs.shape)\r\n    print("Initial Info:", info)\r\n\r\n    for _ in range(5):\r\n        action = env.action_space.sample() # Random action\r\n        obs, reward, terminated, truncated, info = env.step(action)\r\n        print(f"Step: Reward={reward:.2f}, Terminated={terminated}, Truncated={truncated}")\r\n        if terminated or truncated:\r\n            break\r\n    env.close()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"explanation",children:"Explanation"}),"\n",(0,i.jsx)(n.p,{children:"This conceptual Python code defines a basic RL environment:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"gym.Env"})," Structure"]}),": It inherits from ",(0,i.jsx)(n.code,{children:"gym.Env"})," to provide a standardized interface for RL agents."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Space"}),": Defines actions as continuous end-effector movements and a discrete gripper command."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Observation Space"}),": Specifies the types of information the agent receives (e.g., robot pose, object pose, joint angles)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"reset()"})}),": Resets the simulation to an initial state, often randomizing object positions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"step(action)"})}),": Applies an action to the simulated robot, advances the simulation, and returns the next observation, reward, and termination flags."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"_compute_reward()"})}),": Defines how the agent is rewarded for its actions (e.g., positive for grasping, negative for distance)."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"high-level-algorithm-ppo---proximal-policy-optimization",children:"High-Level Algorithm (PPO - Proximal Policy Optimization)"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"examples/rl_sim_to_real/rl_environment_config.yaml"})," suggests using PPO. A typical PPO training loop would conceptually look like this:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Initialize"}),": Create the ",(0,i.jsx)(n.code,{children:"ConceptualIsaacGraspingEnv"})," and a PPO agent."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Loop Episodes"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"observation, info = env.reset()"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Loop Timesteps"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"action = agent.select_action(observation)"})," (based on current policy)"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"next_observation, reward, terminated, truncated, info = env.step(action)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"agent.store_experience(observation, action, reward, next_observation, terminated)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"observation = next_observation"})}),"\n",(0,i.jsxs)(n.li,{children:["If enough experience collected, ",(0,i.jsx)(n.code,{children:"agent.update_policy()"})," (using PPO algorithm)"]}),"\n",(0,i.jsxs)(n.li,{children:["If ",(0,i.jsx)(n.code,{children:"terminated"})," or ",(0,i.jsx)(n.code,{children:"truncated"}),", break inner loop."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Evaluate"}),": Periodically evaluate the agent's performance."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"reference",children:"Reference"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"examples/rl_sim_to_real/rl_environment_config.yaml"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var o=r(6540);const i={},t=o.createContext(i);function s(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);