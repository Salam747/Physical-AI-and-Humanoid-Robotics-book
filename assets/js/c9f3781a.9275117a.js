"use strict";(globalThis.webpackChunkrobotic_book=globalThis.webpackChunkrobotic_book||[]).push([[165],{756:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>s,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"module4-vla/llm-integration-for-planning","title":"LLM Integration for High-Level Planning","description":"This chapter explores how Large Language Models (LLMs) can be integrated into a Vision-Language-Action (VLA) pipeline to perform high-level task planning for robots. The focus is on translating abstract natural language commands into a sequence of executable robot actions.","source":"@site/docs/module4-vla/02-llm-integration-for-planning.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/llm-integration-for-planning","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/llm-integration-for-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/yourusername/physical-ai-humanoid-book/tree/main/docs/module4-vla/02-llm-integration-for-planning.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"VLA Pipeline Architecture","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/vla-pipeline-architecture"},"next":{"title":"Vision and Language Processing for VLA","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/vision-language-processing"}}');var t=o(4848),a=o(8453);const r={},i="LLM Integration for High-Level Planning",s={},c=[{value:"Role of LLMs in Robotics Planning",id:"role-of-llms-in-robotics-planning",level:2},{value:"Architectures for LLM-driven Planning",id:"architectures-for-llm-driven-planning",level:2},{value:"Integrating LLMs with ROS 2",id:"integrating-llms-with-ros-2",level:2},{value:"Example: LLM-driven Object Retrieval",id:"example-llm-driven-object-retrieval",level:2},{value:"<code>llm_planner.py</code>",id:"llm_plannerpy",level:3},{value:"Explanation",id:"explanation",level:3},{value:"How to Run",id:"how-to-run",level:3},{value:"Reference",id:"reference",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"llm-integration-for-high-level-planning",children:"LLM Integration for High-Level Planning"})}),"\n",(0,t.jsx)(e.p,{children:"This chapter explores how Large Language Models (LLMs) can be integrated into a Vision-Language-Action (VLA) pipeline to perform high-level task planning for robots. The focus is on translating abstract natural language commands into a sequence of executable robot actions."}),"\n",(0,t.jsx)(e.h2,{id:"role-of-llms-in-robotics-planning",children:"Role of LLMs in Robotics Planning"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Understanding Natural Language"}),": Interpreting user commands and extracting intents."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking down complex goals into smaller, manageable sub-tasks."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Common Sense Reasoning"}),": Leveraging LLM's vast knowledge for context-aware decision-making."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptability"}),": Generating plans for novel situations not explicitly seen during training."]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"architectures-for-llm-driven-planning",children:"Architectures for LLM-driven Planning"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Prompt Engineering"}),": Crafting effective prompts to guide LLM behavior."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Tool Use (Function Calling)"}),": Enabling LLMs to interact with external tools (e.g., motion planners, vision APIs)."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hierarchical Planning"}),": LLMs generate high-level plans, which are then refined by classical planners or lower-level controllers."]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"integrating-llms-with-ros-2",children:"Integrating LLMs with ROS 2"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS 2 Clients"}),": Developing Python nodes to communicate with LLM APIs."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Servers"}),": Exposing robot capabilities as ROS 2 Actions for LLM control."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Message Definitions"}),": Designing custom ROS 2 messages for LLM input/output."]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"example-llm-driven-object-retrieval",children:"Example: LLM-driven Object Retrieval"}),"\n",(0,t.jsxs)(e.p,{children:["This example utilizes our conceptual ",(0,t.jsx)(e.code,{children:"examples/llm_robot_control/llm_planner.py"}),' script to demonstrate how an LLM can receive a natural language command ("pick up red ball"), decompose it into a high-level plan, and then translate that plan into low-level robot actions.']}),"\n",(0,t.jsx)(e.h3,{id:"llm_plannerpy",children:(0,t.jsx)(e.code,{children:"llm_planner.py"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# examples/llm_robot_control/llm_planner.py\r\nimport openai # or any other LLM API client\r\nimport json\r\n\r\ndef get_llm_plan(natural_language_command, current_robot_state):\r\n    """\r\n    Simulates an LLM generating a high-level plan from a natural language command.\r\n    """\r\n    prompt = f"Given the robot state: {current_robot_state}, generate a high-level plan to execute the command: \'{natural_language_command}\'. " \\\r\n             "Output the plan as a JSON list of actions, where each action is a dictionary with \'name\' and \'params\'."\r\n\r\n    # This is a placeholder for actual LLM API call\r\n    # response = openai.Completion.create(engine="davinci", prompt=prompt, max_tokens=100)\r\n    # llm_output = response.choices[0].text.strip()\r\n\r\n    # Simulate LLM output for demonstration\r\n    if "pick up red ball" in natural_language_command.lower():\r\n        llm_output = """\r\n        [\r\n            {"name": "move_to_object", "params": {"object_id": "red_ball"}},\r\n            {"name": "grasp_object", "params": {"object_id": "red_ball"}},\r\n            {"name": "move_to_location", "params": {"location": "drop_zone"}}\r\n        ]\r\n        """\r\n    else:\r\n        llm_output = """\r\n        [\r\n            {"name": "explore_environment", "params": {}},\r\n            {"name": "wait", "params": {"duration": 5}}\r\n        ]\r\n        """\r\n    \r\n    try:\r\n        plan = json.loads(llm_output)\r\n        return plan\r\n    except json.JSONDecodeError:\r\n        print("LLM output was not valid JSON.")\r\n        return []\r\n\r\ndef translate_plan_to_robot_actions(llm_plan):\r\n    """\r\n    Translates high-level LLM plan into low-level robot actions.\r\n    """\r\n    robot_actions = []\r\n    for action in llm_plan:\r\n        if action["name"] == "move_to_object":\r\n            robot_actions.append(f"Execute: move_arm_to_object({action[\'params\'][\'object_id\']})")\r\n        elif action["name"] == "grasp_object":\r\n            robot_actions.append(f"Execute: close_gripper({action[\'params\'][\'object_id\']})")\r\n        elif action["name"] == "move_to_location":\r\n            robot_actions.append(f"Execute: navigate_to_waypoint({action[\'params\'][\'location\']})")\r\n        else:\r\n            robot_actions.append(f"Execute: {action[\'name\']}({action[\'params\']})")\r\n    return robot_actions\r\n\r\nif __name__ == "__main__":\r\n    command = "Please pick up the red ball and place it on the table."\r\n    state = {"objects_in_view": ["red_ball", "blue_cube"], "robot_position": "near_table"}\r\n    \r\n    high_level_plan = get_llm_plan(command, state)\r\n    print("High-Level LLM Plan:", high_level_plan)\r\n    \r\n    low_level_actions = translate_plan_to_robot_actions(high_level_plan)\r\n    print("Low-Level Robot Actions:", low_level_actions)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"explanation",children:"Explanation"}),"\n",(0,t.jsx)(e.p,{children:"This script demonstrates the conceptual flow:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.code,{children:"get_llm_plan()"})}),': This function simulates an LLM call. It takes a natural language command and the robot\'s current state (e.g., objects in view, robot position). Based on this input, it "generates" a high-level plan as a JSON list of actions. In a real system, this would be an API call to an LLM.']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.code,{children:"translate_plan_to_robot_actions()"})}),': This function takes the LLM\'s high-level plan and translates it into more specific, low-level robot actions. For instance, "move_to_object" becomes a call to ',(0,t.jsx)(e.code,{children:"move_arm_to_object()"}),". This translation layer is crucial for safety and ensuring the LLM's abstract commands are executable."]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"how-to-run",children:"How to Run"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["Ensure you have ",(0,t.jsx)(e.code,{children:"examples/llm_robot_control/llm_planner.py"})," in your project."]}),"\n",(0,t.jsxs)(e.li,{children:["Run the script from your terminal:","\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"python3 examples/llm_robot_control/llm_planner.py\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"You will see the simulated LLM plan and the resulting low-level robot actions printed to the console."}),"\n",(0,t.jsx)(e.h3,{id:"reference",children:"Reference"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.code,{children:"examples/llm_robot_control/llm_planner.py"})}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,o)=>{o.d(e,{R:()=>r,x:()=>i});var l=o(6540);const t={},a=l.createContext(t);function r(n){const e=l.useContext(a);return l.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),l.createElement(a.Provider,{value:e},n.children)}}}]);