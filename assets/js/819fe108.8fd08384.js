"use strict";(globalThis.webpackChunkrobotic_book=globalThis.webpackChunkrobotic_book||[]).push([[49],{401:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>l,metadata:()=>t,toc:()=>s});const t=JSON.parse('{"id":"module4-vla/robot-action-execution","title":"Robot Action Execution from LLM Plans","description":"This chapter focuses on the crucial final step of the Vision-Language-Action (VLA) pipeline: translating the high-level plans generated by Large Language Models (LLMs) into concrete, executable robot movements and manipulations. This involves leveraging existing robot control frameworks and ensuring safe, reliable execution in the physical world.","source":"@site/docs/module4-vla/04-robot-action-execution.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/robot-action-execution","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/robot-action-execution","draft":false,"unlisted":false,"editUrl":"https://github.com/yourusername/physical-ai-humanoid-book/tree/main/docs/module4-vla/04-robot-action-execution.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Vision and Language Processing for VLA","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/vision-language-processing"},"next":{"title":"Capstone Project: End-to-End VLA System","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/capstone-project"}}');var a=o(4848),i=o(8453);const l={},r="Robot Action Execution from LLM Plans",c={},s=[{value:"Bridging LLM Plans to Robot Control",id:"bridging-llm-plans-to-robot-control",level:2},{value:"ROS 2 Integration for Action Execution",id:"ros-2-integration-for-action-execution",level:2},{value:"Safety and Robustness Considerations",id:"safety-and-robustness-considerations",level:2},{value:"Example: Executing a &quot;Pick and Place&quot; Plan (Conceptual ROS 2 Actions)",id:"example-executing-a-pick-and-place-plan-conceptual-ros-2-actions",level:2},{value:"1. Conceptual ROS 2 Action Clients (<code>conceptual_action_executor.py</code>)",id:"1-conceptual-ros-2-action-clients-conceptual_action_executorpy",level:3},{value:"Explanation",id:"explanation",level:3},{value:"How to Run",id:"how-to-run",level:3},{value:"Reference",id:"reference",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"robot-action-execution-from-llm-plans",children:"Robot Action Execution from LLM Plans"})}),"\n",(0,a.jsx)(n.p,{children:"This chapter focuses on the crucial final step of the Vision-Language-Action (VLA) pipeline: translating the high-level plans generated by Large Language Models (LLMs) into concrete, executable robot movements and manipulations. This involves leveraging existing robot control frameworks and ensuring safe, reliable execution in the physical world."}),"\n",(0,a.jsx)(n.h2,{id:"bridging-llm-plans-to-robot-control",children:"Bridging LLM Plans to Robot Control"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Primitives"}),': Defining a set of basic, low-level robot capabilities (e.g., "move_to_pose", "grasp_object", "open_gripper").']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Plan Translation"}),': Converting the LLM\'s high-level task plan (e.g., "pick up the red cube") into a sequence of these action primitives.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State Machines"}),": Implementing state-based logic to manage the execution flow of complex robot tasks."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Handling"}),": Detecting and recovering from execution failures (e.g., object not found, grasp failed)."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"ros-2-integration-for-action-execution",children:"ROS 2 Integration for Action Execution"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS 2 Actions"}),": Utilizing the ROS 2 Action interface for goal-based, asynchronous task execution."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"MoveIt 2"}),": Integrating with MoveIt 2 for motion planning, inverse kinematics, and collision avoidance for robotic arms."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Navigation2"}),": Using Nav2 for autonomous mobile robot navigation."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception Feedback"}),": Incorporating real-time sensor data to refine execution and confirm task completion."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"safety-and-robustness-considerations",children:"Safety and Robustness Considerations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Collision Avoidance"}),": Ensuring robot movements do not result in collisions with the environment or humans."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Emergency Stop"}),": Implementing mechanisms for immediate robot shutdown."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Human-Robot Collaboration"}),": Designing safe interaction protocols."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Uncertainty Handling"}),": Dealing with imperfect sensor data and dynamic environments."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"example-executing-a-pick-and-place-plan-conceptual-ros-2-actions",children:'Example: Executing a "Pick and Place" Plan (Conceptual ROS 2 Actions)'}),"\n",(0,a.jsxs)(n.p,{children:['This conceptual example demonstrates how an LLM-generated high-level plan for a "pick and place" task can be translated into and executed using ROS 2 Action Clients. We\'ll simulate the interaction with an (unimplemented) ROS 2 Action Server that performs the actual robot movements. This builds upon the ',(0,a.jsx)(n.code,{children:"examples/llm_robot_control/llm_planner.py"})," concept."]}),"\n",(0,a.jsxs)(n.h3,{id:"1-conceptual-ros-2-action-clients-conceptual_action_executorpy",children:["1. Conceptual ROS 2 Action Clients (",(0,a.jsx)(n.code,{children:"conceptual_action_executor.py"}),")"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Conceptual Python script for executing LLM-generated plans via ROS 2 Actions\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom action_interfaces.action import PickAndPlace, MoveArm  # Conceptual custom action types\r\nimport json\r\nimport time\r\n\r\n# Assuming action_interfaces.action.PickAndPlace and MoveArm exist\r\n# For demonstration, we\'ll simulate these.\r\n\r\nclass ConceptualActionExecutor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'conceptual_action_executor\')\r\n        self.get_logger().info("Conceptual Action Executor Node Started.")\r\n\r\n        # Conceptual Action Clients\r\n        self._pick_and_place_action_client = ActionClient(self, PickAndPlace, \'pick_and_place_robot\')\r\n        self._move_arm_action_client = ActionClient(self, MoveArm, \'move_arm_robot\')\r\n\r\n    def send_pick_and_place_goal(self, object_name, target_location):\r\n        self.get_logger().info(f"Sending PickAndPlace goal: {object_name} to {target_location}")\r\n        goal_msg = PickAndPlace.Goal()\r\n        goal_msg.object_name = object_name\r\n        goal_msg.target_location = target_location\r\n\r\n        # self._pick_and_place_action_client.wait_for_server() # In real system\r\n        # self._send_goal_future = self._pick_and_place_action_client.send_goal_async(goal_msg)\r\n        # self._send_goal_future.add_done_callback(self.pick_and_place_response_callback)\r\n        \r\n        # Simulate action execution\r\n        time.sleep(2)\r\n        self.get_logger().info(f"Simulated: Picked \'{object_name}\' and placed at \'{target_location}\'.")\r\n        return True # Simulate success\r\n\r\n    def send_move_arm_goal(self, target_pose):\r\n        self.get_logger().info(f"Sending MoveArm goal to pose: {target_pose}")\r\n        goal_msg = MoveArm.Goal()\r\n        goal_msg.target_pose = target_pose\r\n\r\n        # self._move_arm_action_client.wait_for_server() # In real system\r\n        # self._send_goal_future = self._move_arm_action_client.send_goal_async(goal_msg)\r\n        # self._send_goal_future.add_done_callback(self.move_arm_response_callback)\r\n        \r\n        # Simulate action execution\r\n        time.sleep(1)\r\n        self.get_logger().info(f"Simulated: Moved arm to {target_pose}.")\r\n        return True # Simulate success\r\n\r\n    def execute_llm_plan(self, llm_plan):\r\n        self.get_logger().info(f"\\nExecuting LLM Plan: {llm_plan}")\r\n        for action in llm_plan:\r\n            action_name = action["name"]\r\n            params = action["params"]\r\n            if action_name == "move_to_object":\r\n                # In a real system, this would translate to a MoveArm action client call\r\n                self.get_logger().info(f"Translating \'move_to_object\' for {params[\'object_id\']} to MoveArm action.")\r\n                # self.send_move_arm_goal(get_pose_of_object(params[\'object_id\'])) # Get pose from vision\r\n                time.sleep(1) # Simulate movement\r\n            elif action_name == "grasp_object":\r\n                self.get_logger().info(f"Translating \'grasp_object\' for {params[\'object_id\']} to PickAndPlace action (grasp part).")\r\n                # self.send_pick_and_place_goal(params[\'object_id\'], "current_location") # Just grasp\r\n                time.sleep(1) # Simulate grasp\r\n            elif action_name == "move_to_location":\r\n                self.get_logger().info(f"Translating \'move_to_location\' for {params[\'location\']} to MoveArm action (navigate part).")\r\n                # self.send_move_arm_goal(get_pose_of_location(params[\'location\'])) # Get pose of location\r\n                time.sleep(2) # Simulate movement\r\n            elif action_name == "pick_and_place": # Hypothetical combined action\r\n                self.send_pick_and_place_goal(params[\'object_id\'], params[\'target_location\'])\r\n            else:\r\n                self.get_logger().warning(f"Unknown action: {action_name}")\r\n            \r\n        self.get_logger().info("LLM Plan execution complete.")\r\n\r\n# Placeholder for conceptual Action interfaces\r\nclass PickAndPlace:\r\n    class Goal:\r\n        object_name = ""\r\n        target_location = ""\r\n    # Result, Feedback classes would also be here\r\n\r\nclass MoveArm:\r\n    class Goal:\r\n        target_pose = {} # e.g., {\'x\': 0, \'y\': 0, \'z\': 0}\r\n    # Result, Feedback classes would also be here\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    action_executor = ConceptualActionExecutor()\r\n\r\n    # Simulate an LLM-generated plan (from examples/llm_robot_control/llm_planner.py)\r\n    llm_plan_example = [\r\n        {"name": "move_to_object", "params": {"object_id": "red_ball"}},\r\n        {"name": "grasp_object", "params": {"object_id": "red_ball"}},\r\n        {"name": "move_to_location", "params": {"location": "drop_zone"}}\r\n    ]\r\n\r\n    action_executor.execute_llm_plan(llm_plan_example)\r\n    \r\n    action_executor.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"explanation",children:"Explanation"}),"\n",(0,a.jsx)(n.p,{children:"This script demonstrates the Action Execution module's role:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Conceptual Action Clients"}),": It sets up ",(0,a.jsx)(n.code,{children:"ActionClient"})," objects for hypothetical ",(0,a.jsx)(n.code,{children:"PickAndPlace"})," and ",(0,a.jsx)(n.code,{children:"MoveArm"})," ROS 2 actions. In a real system, these would connect to actual Action Servers on the robot."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"execute_llm_plan()"})}),': This function iterates through a simulated LLM plan (a list of high-level actions like "move_to_object", "grasp_object"). For each high-level action, it conceptually translates it into a call to the appropriate ROS 2 Action Client.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulated Action Execution"}),": Instead of actual robot movement, ",(0,a.jsx)(n.code,{children:"time.sleep()"})," is used to simulate the duration of these actions, and ",(0,a.jsx)(n.code,{children:"self.get_logger().info()"})," prints messages about the simulated execution."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This module acts as the bridge, converting the LLM's abstract intent into concrete physical movements by calling the robot's pre-defined capabilities via ROS 2 Actions."}),"\n",(0,a.jsx)(n.h3,{id:"how-to-run",children:"How to Run"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Save the above content as ",(0,a.jsx)(n.code,{children:"conceptual_action_executor.py"})," (e.g., in ",(0,a.jsx)(n.code,{children:"examples/llm_robot_control/"}),")."]}),"\n",(0,a.jsx)(n.li,{children:"Ensure you have a ROS 2 environment sourced."}),"\n",(0,a.jsxs)(n.li,{children:["Run the script from your terminal:","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python3 conceptual_action_executor.py\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You will see the simulated execution of the LLM-generated plan, showing the translation of high-level actions into conceptual robot commands."}),"\n",(0,a.jsx)(n.h3,{id:"reference",children:"Reference"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"examples/llm_robot_control/llm_planner.py"})," (provides the LLM plan example)"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>l,x:()=>r});var t=o(6540);const a={},i=t.createContext(a);function l(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);