"use strict";(globalThis.webpackChunkrobotic_book=globalThis.webpackChunkrobotic_book||[]).push([[233],{1515:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module3-ai-brain/sim-to-real","title":"Sim-to-Real Transfer Recipes","description":"This chapter focuses on the critical problem of Sim-to-Real (S2R) transfer in robotics \u2013 how to successfully deploy policies or behaviors learned in simulation to physical robots. You will learn practical recipes and techniques to minimize the \\"reality gap\\" and ensure your AI models perform reliably in the real world.","source":"@site/docs/module3-ai-brain/07-sim-to-real.mdx","sourceDirName":"module3-ai-brain","slug":"/module3-ai-brain/sim-to-real","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/sim-to-real","draft":false,"unlisted":false,"editUrl":"https://github.com/yourusername/physical-ai-humanoid-book/tree/main/docs/module3-ai-brain/07-sim-to-real.mdx","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning Basics for Robotics","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module3-ai-brain/reinforcement-learning-basics"},"next":{"title":"Module 4 \u2013 Vision-Language-Action (VLA)","permalink":"/Physical-AI-and-Humanoid-Robotics-book/docs/module4-vla/"}}');var o=i(4848),a=i(8453);const t={},s="Sim-to-Real Transfer Recipes",l={},c=[{value:"The Reality Gap Revisited",id:"the-reality-gap-revisited",level:2},{value:"Key Sim-to-Real Techniques",id:"key-sim-to-real-techniques",level:2},{value:"Sim-to-Real Transfer for Humanoid Robots",id:"sim-to-real-transfer-for-humanoid-robots",level:2},{value:"Example: Transferring a Grasping Policy (Conceptual Sim-to-Real Pipeline)",id:"example-transferring-a-grasping-policy-conceptual-sim-to-real-pipeline",level:2},{value:"1. Conceptual Sim-to-Real Pipeline (<code>conceptual_s2r_pipeline.py</code>)",id:"1-conceptual-sim-to-real-pipeline-conceptual_s2r_pipelinepy",level:3},{value:"Explanation",id:"explanation",level:3},{value:"References",id:"references",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"sim-to-real-transfer-recipes",children:"Sim-to-Real Transfer Recipes"})}),"\n",(0,o.jsx)(n.p,{children:'This chapter focuses on the critical problem of Sim-to-Real (S2R) transfer in robotics \u2013 how to successfully deploy policies or behaviors learned in simulation to physical robots. You will learn practical recipes and techniques to minimize the "reality gap" and ensure your AI models perform reliably in the real world.'}),"\n",(0,o.jsx)(n.h2,{id:"the-reality-gap-revisited",children:"The Reality Gap Revisited"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sources of Discrepancy"}),": Differences in physics, sensor noise, latency, and hardware characteristics between simulation and reality."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Impact on Performance"}),": Models trained in simulation often fail or degrade significantly when deployed to real robots."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-sim-to-real-techniques",children:"Key Sim-to-Real Techniques"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Domain Randomization (DR)"}),": (Brief recap) Randomizing simulation parameters during training to encourage robustness to real-world variations."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Domain Adaptation (DA)"}),": Techniques that explicitly adapt models from a source domain (sim) to a target domain (real) using unpaired data.","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Unsupervised Domain Adaptation"}),": Learning domain-invariant features."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Generative Adversarial Networks (GANs)"}),": Bridging the gap in appearance."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"System Identification"}),": More accurately modeling the physical properties of the real robot and environment in simulation."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Curriculum Learning"}),": Gradually increasing the complexity of the simulation environment during training."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Space and Observation Space Matching"}),": Ensuring consistency between sim and real interfaces."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"sim-to-real-transfer-for-humanoid-robots",children:"Sim-to-Real Transfer for Humanoid Robots"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Proprioceptive Data"}),": Matching joint angles, velocities, and torques."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contact Dynamics"}),": Fine-tuning friction and contact parameters for walking/grasping."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor Noise"}),": Accurately modeling real-world sensor imperfections."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"example-transferring-a-grasping-policy-conceptual-sim-to-real-pipeline",children:"Example: Transferring a Grasping Policy (Conceptual Sim-to-Real Pipeline)"}),"\n",(0,o.jsx)(n.p,{children:"This conceptual example outlines a pipeline for successfully transferring a robotic grasping policy trained in simulation (e.g., Isaac Sim with Domain Randomization) to a real robot. We will refer to configuration files and scripts that conceptually represent different stages of this process."}),"\n",(0,o.jsxs)(n.h3,{id:"1-conceptual-sim-to-real-pipeline-conceptual_s2r_pipelinepy",children:["1. Conceptual Sim-to-Real Pipeline (",(0,o.jsx)(n.code,{children:"conceptual_s2r_pipeline.py"}),")"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Conceptual Python script for Sim-to-Real Transfer Pipeline\r\nimport json\r\nimport os\r\n# import omni.isaac.core as icore # For simulation\r\n# import your_robot_hardware_interface # For real robot\r\n\r\ndef configure_simulation_environment(config_path):\r\n    """Configures the simulation environment, potentially with domain randomization."""\r\n    print(f"Loading RL environment configuration from {config_path}")\r\n    with open(config_path, \'r\') as f:\r\n        config = json.load(f)\r\n    \r\n    print(f"  Environment: {config[\'environment\'][\'name\']}")\r\n    print(f"  Randomization factor: {config[\'environment\'][\'randomization\'][\'domain_rand_factor\']}")\r\n    # icore.initialize() # Initialize Isaac Sim\r\n    # world = icore.World()\r\n    # Apply randomization settings from config\r\n    # return world\r\n\r\ndef train_policy_in_simulation():\r\n    """Trains a robot policy (e.g., grasping) within the configured simulation."""\r\n    print("Training policy in simulation (e.g., Isaac Sim) using PPO algorithm...")\r\n    # rl_agent = PPOAgent()\r\n    # rl_agent.train(simulation_environment)\r\n    print("Policy training complete. Trained model: trained_policy_sim.pth")\r\n    return "trained_policy_sim.pth"\r\n\r\ndef evaluate_sim_performance(policy_model):\r\n    """Evaluates the trained policy\'s performance in simulation."""\r\n    print(f"Evaluating policy \'{policy_model}\' performance in simulation...")\r\n    # simulation_success_rate = rl_agent.evaluate(policy_model, simulation_environment)\r\n    simulation_success_rate = 0.95 # Conceptual success rate\r\n    print(f"Simulation success rate: {simulation_success_rate:.2f}")\r\n    return simulation_success_rate\r\n\r\ndef perform_system_identification():\r\n    """Performs system identification on the real robot to reduce the reality gap."""\r\n    print("Performing system identification on real robot...")\r\n    # Measure real robot\'s friction, inertia, sensor noise, etc.\r\n    # Update simulation parameters based on real-world measurements\r\n    print("System identification complete. Simulation parameters adjusted.")\r\n\r\ndef transfer_and_deploy_policy(policy_model):\r\n    """Transfers the trained policy to the real robot and deploys it."""\r\n    print(f"Transferring and deploying policy \'{policy_model}\' to real robot...")\r\n    # This might involve model conversion (e.g., ONNX, TensorRT) for edge deployment\r\n    # And then using a deployment script like sim_to_real_transfer_script.py\r\n    \r\n    # from examples.rl_sim_to_real import sim_to_real_transfer_script\r\n    # sim_to_real_transfer_script.main(policy_model) # Conceptual call\r\n    print("Policy deployed to real robot.")\r\n\r\ndef evaluate_real_performance():\r\n    """Evaluates the deployed policy\'s performance on the real robot."""\r\n    print("Evaluating real robot performance...")\r\n    # real_success_rate = real_robot.evaluate_policy()\r\n    real_success_rate = 0.88 # Conceptual success rate\r\n    print(f"Real-world success rate: {real_success_rate:.2f}")\r\n    return real_success_rate\r\n\r\ndef main():\r\n    print("--- Conceptual Sim-to-Real Pipeline ---")\r\n    \r\n    # Stage 1: Configure Simulation & Train\r\n    # sim_world = configure_simulation_environment("examples/rl_sim_to_real/rl_environment_config.yaml")\r\n    # trained_policy = train_policy_in_simulation()\r\n    \r\n    # For demonstration, assume policy is trained\r\n    trained_policy_model = "trained_policy_sim.pth" \r\n\r\n    # Stage 2: Evaluate Sim Performance\r\n    sim_rate = evaluate_sim_performance(trained_policy_model)\r\n\r\n    # Stage 3: Reduce Reality Gap & Deploy\r\n    # perform_system_identification()\r\n    transfer_and_deploy_policy(trained_policy_model)\r\n\r\n    # Stage 4: Evaluate Real Performance\r\n    real_rate = evaluate_real_performance()\r\n\r\n    print(f"\\n--- Sim-to-Real Transfer Summary ---")\r\n    print(f"Simulation Success Rate: {sim_rate:.2f}")\r\n    print(f"Real-world Success Rate: {real_rate:.2f}")\r\n    print(f"Performance Drop: {(sim_rate - real_rate):.2f}")\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"explanation",children:"Explanation"}),"\n",(0,o.jsx)(n.p,{children:"This conceptual pipeline demonstrates the stages of a sim-to-real transfer for an RL policy:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation Configuration & Training"}),": The ",(0,o.jsx)(n.code,{children:"configure_simulation_environment"})," function conceptually sets up an Isaac Sim environment, potentially applying domain randomization as defined in ",(0,o.jsx)(n.code,{children:"rl_environment_config.yaml"}),". The ",(0,o.jsx)(n.code,{children:"train_policy_in_simulation"})," then represents the process of training a robot policy (e.g., for grasping) within this environment."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation Evaluation"}),": ",(0,o.jsx)(n.code,{children:"evaluate_sim_performance"})," assesses how well the policy performs in the simulated world."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reality Gap Reduction & Deployment"}),": ",(0,o.jsx)(n.code,{children:"perform_system_identification"})," conceptually represents a step to measure real-world physical properties and update simulation parameters to reduce discrepancies. ",(0,o.jsx)(n.code,{children:"transfer_and_deploy_policy"})," then takes the trained policy and deploys it to the physical robot, often involving optimizations for edge hardware."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-world Evaluation"}),": Finally, ",(0,o.jsx)(n.code,{children:"evaluate_real_performance"})," measures the policy's success rate on the actual robot."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'The goal is to minimize the "Performance Drop" between simulation and reality, validating the effectiveness of the sim-to-real techniques used.'}),"\n",(0,o.jsx)(n.h3,{id:"references",children:"References"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"examples/rl_sim_to_real/rl_environment_config.yaml"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"examples/rl_sim_to_real/sim_to_real_transfer_script.py"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>s});var r=i(6540);const o={},a=r.createContext(o);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);